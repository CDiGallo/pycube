{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e03089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing French dataset name\n",
      "Missing Italian dataset name\n",
      "Missing English dataset name\n",
      "Missing French dataset description\n",
      "Missing Italian dataset description\n",
      "Missing English dataset description\n",
      "Missing source modification date\n",
      "Visualize work example link is not correctly formatted\n",
      "SPARQL work example link is missing\n",
      "dataset creator not formatted correctly\n",
      "dataset contributor not formatted correctly\n",
      "dataset publisher not formatted correctly\n",
      "Optional: no next modification date provided\n",
      "is it a draft or published? creativeworkstatus wrong\n",
      "opendata.swiss creator was not formatted correctly\n",
      "opendata.swiss license missing\n",
      "opendata.swiss accrual periodicity missing\n",
      "opendata.swiss theme was not provided\n",
      "opendata.swiss german keywords were not provided\n",
      "opendata.swiss french keywords were not provided\n",
      "opendata.swiss italian keywords were not provided\n",
      "opendata.swiss english keywords were not provided\n",
      "opendata.swiss languages missing\n",
      "opendata.swiss landing page missing\n",
      "Optional: no opendata.swiss documentation pages provided\n",
      "Optional: no opendata.swiss related resource provided\n",
      "Optional: no opendata.swiss qualified relation provided\n",
      "Jahr\n",
      "N2449be9bb33645fb8c31e58c26f48405\n",
      "Katon\n",
      "Nc5d567d906294019865129f23bb7e23d\n",
      "Anzahl gefundene Kleebl√§tter\n",
      "N71aaf1437d924f6099f8813b22b346f4\n",
      "Anzahl Lottogewinne\n",
      "Nbeb00680897e4caaacd9d0a70d5df700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nddbb51d49d554c0488c4dae05ecd1807 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import RDF\n",
    "from dateutil.parser import parse\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "#instantiate namespace\n",
    "schema = Namespace(\"http://schema.org/\")\n",
    "dcat = Namespace(\"http://www.w3.org/ns/dcat#\")\n",
    "dct = Namespace(\"http://purl.org/dc/terms/\")\n",
    "void= Namespace(\"http://rdfs.org/ns/void#\")\n",
    "cube= Namespace(\"https://cube.link/\")\n",
    "shacl= Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "xsd= Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "qudt= Namespace(\"http://qudt.org/schema/qudt/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#parse input\n",
    "with open(\"C:\\\\Users\\\\claudio\\\\Documents\\\\Projekte\\\\python lottogewinne\\\\input_Form.yml\", \"rt\", encoding='utf8') as yml_input:\n",
    "    input_data = yaml.load(yml_input, yaml.Loader)\n",
    "    \n",
    "try:\n",
    "    with open(\"C:\\\\Users\\\\claudio\\\\Documents\\\\Projekte\\\\python lottogewinne\\\\ODS_Form.yml\", \"rt\", encoding='utf8') as ods_input:\n",
    "        ODS_data = yaml.load(ods_input, yaml.Loader)\n",
    "    if ODS_data.get(\"identifier\"):\n",
    "        ODS_flag = True\n",
    "    else:\n",
    "        ODS_flag = False\n",
    "        print(\"No opendata.swiss metadata was provided\")\n",
    "except(FileNotFoundError):\n",
    "    print(\"No opendata.swiss metadata form was provided\")\n",
    "\n",
    "\n",
    "\n",
    "#instantiate graph\n",
    "g = Graph()\n",
    "\n",
    "#could add the option to read an existing graph (for example a generated cube constraint and observation set)\n",
    "\n",
    "#set dataset URI to attach metadata to\n",
    "dataset_URL = input_data.get(\"dataset-URI\")\n",
    "#if dataset_URL[-1] != \"/\":\n",
    "#    dataset_URL = dataset_URL+\"/\" # To do check, if this is correct (probably not) if reversed cube-constrain and cube observation is broken (under Dimension)\n",
    "dataset = URIRef(dataset_URL)\n",
    "g.add((dataset, RDF.type, schema.Dataset))\n",
    "#if ODS_flag:\n",
    "g.add((dataset, RDF.type, dcat.Dataset))\n",
    "g.add((dataset, RDF.type, void.Dataset))\n",
    "g.add((dataset, RDF.type, cube.Cube))\n",
    "\n",
    "#for each metadata point the following steps need to be checked through\n",
    "# 1. get from dict\n",
    "# 2. check rough conformance of datatype if possible\n",
    "# 3. assign URIRef or Literal\n",
    "# 4. write triples\n",
    "\n",
    "#name\n",
    "name_DE = input_data.get(\"name_DE\")\n",
    "if name_DE:\n",
    "    de_name = Literal(name_DE.strip(), lang=\"de\")\n",
    "    g.add((dataset, schema.name, de_name))\n",
    "    if ODS_flag:\n",
    "        g.add((dataset, dct.title, de_name))\n",
    "else:\n",
    "    print(\"Missing German dataset name\")\n",
    "name_FR = input_data.get(\"name_FR\")\n",
    "if name_FR:\n",
    "    fr_name = Literal(name_FR.strip(), lang=\"fr\")\n",
    "    g.add((dataset, schema.name, fr_name))\n",
    "    if ODS_flag:\n",
    "        g.add((dataset, dct.title, fr_name))\n",
    "else:\n",
    "    print(\"Missing French dataset name\")\n",
    "name_IT = input_data.get(\"name_IT\")\n",
    "if name_IT:\n",
    "    it_name = Literal(name_IT.strip(), lang=\"it\")\n",
    "    g.add((dataset, schema.name, it_name))\n",
    "    if ODS_flag:\n",
    "        g.add((dataset, dct.title, it_name))\n",
    "else:\n",
    "    print(\"Missing Italian dataset name\")\n",
    "name_EN = input_data.get(\"name_EN\")\n",
    "if name_EN:\n",
    "    en_name = Literal(name_EN.strip(), lang=\"en\")\n",
    "    g.add((dataset, schema.name, en_name))\n",
    "    if ODS_flag:\n",
    "        g.add((dataset, dct.title, en_name))\n",
    "else:\n",
    "    print(\"Missing English dataset name\")\n",
    "description_DE = input_data.get(\"description_DE\")\n",
    "if description_DE:\n",
    "    de_description = Literal(description_DE.strip(), lang=\"de\")\n",
    "    g.add((dataset, schema.description, de_description))\n",
    "    if ODS_flag:\n",
    "        g.add((dataset, dct.description, de_description))\n",
    "else:\n",
    "    print(\"Missing German dataset description\")\n",
    "description_FR = input_data.get(\"description_FR\")\n",
    "if description_FR:\n",
    "    fr_description = Literal(description_FR.strip(), lang=\"fr\")\n",
    "    g.add((dataset, schema.description, fr_description))\n",
    "    if ODS_flag:\n",
    "        g.add((dataset, dct.description, fr_description))\n",
    "else:\n",
    "    print(\"Missing French dataset description\")\n",
    "description_IT = input_data.get(\"description_IT\")\n",
    "if description_IT:\n",
    "    it_description = Literal(description_IT.strip(), lang=\"it\")\n",
    "    g.add((dataset, schema.description, it_description))\n",
    "    if ODS_flag:\n",
    "        g.add((dataset, dct.description, it_description))\n",
    "else:\n",
    "    print(\"Missing Italian dataset description\")\n",
    "description_EN = input_data.get(\"description_EN\")\n",
    "if description_EN:\n",
    "    en_description = Literal(description_EN.strip(), lang=\"en\")\n",
    "    g.add((dataset, schema.description, en_description))\n",
    "    if ODS_flag:\n",
    "        g.add((dataset, dct.description, en_description))\n",
    "else:\n",
    "    print(\"Missing English dataset description\")\n",
    "\n",
    "#contact point\n",
    "contact_name = input_data.get(\"contact-point name\")\n",
    "contact_mail = input_data.get(\"contact-point email\")\n",
    "if contact_name and contact_mail:\n",
    "    if \"@\" in contact_mail:\n",
    "        contact_point = BNode()\n",
    "        con_name = Literal(contact_name)\n",
    "        con_mail = Literal(contact_mail)\n",
    "        g.add((dataset, schema.contactPoint, contact_point))\n",
    "        g.add((contact_point, schema.name, con_name))\n",
    "        g.add((contact_point, schema.email, con_mail))\n",
    "        if ODS_flag:\n",
    "            ODS_contact = BNode()\n",
    "            g.add((dataset, dcat.contactPoint, ODS_contact))\n",
    "            g.add((ODS_contact, RDF.type, URIRef(\"http://www.w3.org/2006/vcard/ns#Organization\")))\n",
    "            g.add((ODS_contact, URIRef(\"http://www.w3.org/2006/vcard/ns#fn\"), con_name))\n",
    "            g.add((ODS_contact, URIRef(\"http://www.w3.org/2006/vcard/ns#hasEmail\"), con_mail))\n",
    "    else:\n",
    "        print(\"Contact point email address is not in correct format\")\n",
    "elif contact_name:\n",
    "    print(\"Contact Point email address is missing\")\n",
    "else:\n",
    "    print(\"Contact Point name is missing\")\n",
    "\n",
    "#dates: to be tested using datetime\n",
    "date_format = \"%Y-%m-%d\"\n",
    "\n",
    "#creation date\n",
    "creation_date = input_data.get(\"source creation-date\")\n",
    "if creation_date:\n",
    "    try: \n",
    "        datetime.strptime(str(creation_date), date_format)\n",
    "        cre_date = Literal(creation_date, datatype=URIRef('http://www.w3.org/2001/XMLSchema#date'))\n",
    "        g.add((dataset, schema.dateCreated, cre_date))\n",
    "    except(ValueError):\n",
    "        print(\"creation date is not formatted as xsd:date\")\n",
    "else:\n",
    "    print(\"Missing source creation date\")\n",
    "\n",
    "#modification date\n",
    "modification_date = input_data.get(\"source modification-date\")\n",
    "if modification_date:\n",
    "    try: \n",
    "        parse(str(modification_date))\n",
    "        mod_date = Literal(modification_date, datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))\n",
    "        g.add((dataset, schema.dateModified, mod_date))\n",
    "        if ODS_flag:\n",
    "            g.add((dataset, dct.modified, mod_date))\n",
    "    except(ValueError):\n",
    "        print(\"modification date is not formatted as xsd:dateTime\")\n",
    "else:\n",
    "    print(\"Missing source modification date\")\n",
    "\n",
    "#publication date\n",
    "publication_date = input_data.get(\"dataset publication-date\")\n",
    "if publication_date:\n",
    "    try: \n",
    "        datetime.strptime(str(publication_date), date_format)\n",
    "        pub_date = Literal(publication_date, datatype=URIRef('http://www.w3.org/2001/XMLSchema#date'))\n",
    "        g.add((dataset, schema.datePublished, pub_date))\n",
    "        if ODS_flag:\n",
    "            issue_date = parse(str(publication_date))\n",
    "            g.add((dataset, dct.issued, Literal(issue_date, datatype=URIRef('http://www.w3.org/2001/XMLSchema#date'))))\n",
    "    except(ValueError):\n",
    "        print(\"publication date is not formatted as xsd:date\")\n",
    "else:\n",
    "    print(\"Missing dataset publication date\")\n",
    "    \n",
    "#work example application\n",
    "work_example_app = input_data.get(\"work example application\").split(\";\")\n",
    "for app in work_example_app:\n",
    "    if app.strip() == \"visualize\":\n",
    "        visualize_link = URIRef(\"https://ld.admin.ch/application/visualize\")\n",
    "        g.add((dataset, schema.workExample, visualize_link))\n",
    "        #could also be made redundant if automatically added when there is a visualize work example. Left for now in case there are datasets that are on visualize without providing a work example URL\n",
    "    elif app.strip() == \"opendata.swiss\":\n",
    "        ODS_link = URIRef(\"https://ld.admin.ch/application/opendataswiss\")\n",
    "        g.add((dataset, schema.workExample, ODS_link))\n",
    "        #possibly this metadata can be used to determine whether ODS metadata should be added or there could be a separate field in the YAML form for whether ODS publication is desired, which automatically adds this triple as well\n",
    "    else:\n",
    "        if app.strip():\n",
    "            other_app_URI = URIRef(\"https://ld.admin.ch/application/\"+app.strip())\n",
    "            g.add((dataset, schema.workExample, other_app_URI))\n",
    "        else:\n",
    "            pass\n",
    "        #work example application YAML form field is still required even if the main two applications would be automatically added elsewhere in case of other custom applications\n",
    "\n",
    "#work example visualize\n",
    "visualize_URL = input_data.get(\"work example visualize\")\n",
    "if visualize_URL:\n",
    "    if \"https://visualize.admin.ch/\" in visualize_URL:\n",
    "        visualize_example = Literal(visualize_URL)\n",
    "        visualize_BN = BNode()\n",
    "        g.add((dataset, schema.workExample, visualize_BN))\n",
    "        g.add((visualize_BN, schema.url, visualize_example))\n",
    "        g.add((visualize_BN, schema.name, Literal(\"visualize.admin.ch\", lang=\"de\")))\n",
    "        g.add((visualize_BN, schema.name, Literal(\"visualize.admin.ch\", lang=\"fr\")))\n",
    "        g.add((visualize_BN, schema.name, Literal(\"visualize.admin.ch\", lang=\"it\")))\n",
    "        g.add((visualize_BN, schema.name, Literal(\"visualize.admin.ch\", lang=\"en\")))\n",
    "        #This is how Cube Creator formats it, but is it really necessary to add 4 triples with different language tags for the same string?\n",
    "        g.add((visualize_BN, schema.encodingFormat, Literal(\"text/html\", datatype=URIRef('http://www.w3.org/2001/XMLSchema#string'))))\n",
    "        g.add((visualize_BN, RDF.type, schema.CreativeWork))\n",
    "        if visualize_link:\n",
    "            pass\n",
    "        else:\n",
    "            visualize_link = URIRef(\"https://ld.admin.ch/application/visualize\")\n",
    "            g.add((dataset, schema.workExample, visualize_link))\n",
    "    else:\n",
    "        print(\"Visualize work example link is not correctly formatted\")\n",
    "else:\n",
    "    print(\"Visualize work example link is missing\")\n",
    "    \n",
    "#work example SPARQL endpoint\n",
    "example_sparql = input_data.get(\"work example SPARQLendpoint\")\n",
    "if example_sparql:\n",
    "    if \"https://lindas.admin.ch/sparql/\" in example_sparql:\n",
    "        sparql_work = Literal(example_sparql)\n",
    "        sparql_BN = BNode()\n",
    "        g.add((dataset, schema.workExample, sparql_BN))\n",
    "        g.add((sparql_BN, schema.url, sparql_work))\n",
    "        g.add((sparql_BN, schema.name, Literal(\"SPARQL Endpoint mit Vorauswahl des Graph\", lang=\"de\")))\n",
    "        g.add((sparql_BN, schema.name, Literal(\"SPARQL Endpoint avec pr√©s√©lection du graphe\", lang=\"fr\")))\n",
    "        g.add((sparql_BN, schema.name, Literal(\"SPARQL Endpoint con preselezione del grafo\", lang=\"it\")))\n",
    "        g.add((sparql_BN, schema.name, Literal(\"SPARQL Endpoint with graph preselection\", lang=\"en\")))\n",
    "        g.add((sparql_BN, schema.encodingFormat, Literal(\"application/sparql-query\", datatype=URIRef('http://www.w3.org/2001/XMLSchema#string'))))\n",
    "        g.add((sparql_BN, RDF.type, schema.CreativeWork))\n",
    "    else:\n",
    "        print(\"SPARQL work example link is not correctly formatted\")\n",
    "else:\n",
    "    print(\"SPARQL work example link is missing\")\n",
    "\n",
    "#SPARQL endpoint\n",
    "sparql_url = input_data.get(\"SPARQL endpoint\")\n",
    "if sparql_url:\n",
    "    sparql_end = Literal(sparql_url)\n",
    "    end_prop = URIRef(\"http://rdfs.org/ns/void#sparqlEndpoint\")\n",
    "    g.add((dataset, end_prop, sparql_end))\n",
    "else:\n",
    "    print(\"SPARQL endpoint is missing\")\n",
    "    \n",
    "#optional metadata\n",
    "creator = input_data.get(\"dataset creator\") \n",
    "if creator:\n",
    "    if \"http\" in creator:\n",
    "        creator_URI = URIRef(creator)\n",
    "        g.add((dataset, schema.creator, creator_URI))\n",
    "    else:\n",
    "        print(\"dataset creator not formatted correctly\")\n",
    "else:\n",
    "    print(\"Optional: no dataset creator provided\")\n",
    "contributor = input_data.get(\"dataset contributor\")\n",
    "if contributor:\n",
    "    if \"http\" in contributor:\n",
    "        contributor_URI = URIRef(contributor)\n",
    "        g.add((dataset, schema.contributor, contributor_URI))\n",
    "    else:\n",
    "        print(\"dataset contributor not formatted correctly\")\n",
    "else:\n",
    "    print(\"Optional: no dataset contributor provided\") \n",
    "publisher = input_data.get(\"dataset publisher\")\n",
    "if publisher:\n",
    "    if \"http\" in publisher:\n",
    "        publisher_URI = URIRef(publisher)\n",
    "        g.add((dataset, schema.publisher, publisher_URI))\n",
    "    else:\n",
    "        print(\"dataset publisher not formatted correctly\")\n",
    "else:\n",
    "    print(\"Optional: no dataset publisher provided\") \n",
    "\n",
    "next_modification = input_data.get(\"next modification date\") \n",
    "if next_modification:\n",
    "    try: \n",
    "        datetime.strptime(str(next_modification), date_format)\n",
    "        next_mod = Literal(next_modification, datatype=URIRef('http://www.w3.org/2001/XMLSchema#date'))\n",
    "        g.add((dataset, URIRef(\"https://schema.ld.admin.ch/datasetNextDateModified\"), next_mod))\n",
    "    except(ValueError):\n",
    "        print(\"next modification date is not formatted as xsd:dateTime\")\n",
    "    \n",
    "else:\n",
    "    print(\"Optional: no next modification date provided\")\n",
    "\n",
    "example_resource = input_data.get(\"example resource\")\n",
    "if example_resource:\n",
    "    data_URI = \"/\".join(dataset_URL.split(\"/\")[:3])\n",
    "    if data_URI in example_resource:\n",
    "        g.add((dataset, URIRef(\"https://schema.ld.admin.ch/exampleResource\"), URIRef(example_resource)))\n",
    "else:\n",
    "    print(\"Optional: no example resource provided\")\n",
    "\n",
    "work_status = input_data.get(\"creative work status\")\n",
    "if work_status:\n",
    "    try: \n",
    "        g.add((dataset, schema.creativeWorkStatus)), URIRef(work_status)\n",
    "    except(ValueError):\n",
    "        print(\"is it a draft or published? creativeworkstatus wrong\")  \n",
    "   \n",
    "    \n",
    "    \n",
    "# ODS metadata\n",
    "if ODS_flag:\n",
    "    ID = ODS_data.get(\"identifier\")\n",
    "    if \"@\" in ID:\n",
    "        g.add((dataset, dct.identifier, Literal(ID)))\n",
    "    else: print(\"opendata.swiss identifier not formatted correctly\")\n",
    "   \n",
    "    creator_ODS = ODS_data.get(\"creator\")\n",
    "    if creator_ODS:\n",
    "        if \"https://register.ld.admin.ch/opendataswiss/org\" in creator_ODS:\n",
    "            g.add((dataset, dct.creator, URIRef(creator_ODS)))\n",
    "            if not creator:\n",
    "                g.add((dataset, schema.creator, URIRef(creator_ODS)))\n",
    "            else: pass\n",
    "        else: print(\"opendata.swiss creator was not formatted correctly\")\n",
    "    else: print(\"opendata.swiss creator missing\")\n",
    "    publisher_ODS = ODS_data.get(\"publisher\")\n",
    "    if publisher_ODS:\n",
    "        g.add((dataset, dct.publisher, Literal(publisher_ODS)))\n",
    "    else: print(\"opendata.swiss publisher missing\")\n",
    "    license_ODS = ODS_data.get(\"license\")\n",
    "    #this script follows the current implementation of \"dct:license\" on LINDAS rather than the ideal description in the DCAT-AP handbook\n",
    "    if license_ODS:\n",
    "        if \"https://\" and \"fedlex\" and \".admin.ch\" in license_ODS:\n",
    "            g.add((dataset, dct.rights, URIRef(license_ODS)))\n",
    "        else: print(\"opendata.swiss license not formatted correctly\")\n",
    "    else: print(\"opendata.swiss license missing\")\n",
    "    \n",
    "    rights = ODS_data.get(\"rights\")\n",
    "    #this script follows the current implementation of \"dct:rights\" on LINDAS rather than the ideal description in the DCAT-AP handbook\n",
    "    if rights:\n",
    "        if \"https://ld.admin.ch/vocabulary/TermsOfUse\" in rights:\n",
    "            g.add((dataset, dct.rights, URIRef(rights)))\n",
    "        else: print(\"opendata.swiss rights not formatted correctly\")\n",
    "    else: print(\"opendata.swiss rights missing\")\n",
    "    \n",
    "    accrual = ODS_data.get(\"accrual periodicity\")\n",
    "    if accrual:\n",
    "        if \"http://publications.europa.eu/resource/authority/frequency\" in accrual:\n",
    "            g.add((dataset, dct.accrualPeriodicity, Literal(accrual)))\n",
    "        else: print(\"opendata.swiss accrual periodicity not formatted correctly\")\n",
    "    else: print(\"opendata.swiss accrual periodicity missing\")\n",
    "    \n",
    "    themes = ODS_data.get(\"themes\").split(\";\")\n",
    "    if themes[0]:\n",
    "        for theme in themes:\n",
    "            if not theme:\n",
    "                pass\n",
    "            elif \"https://register.ld.admin.ch/opendataswiss/category/\" in theme.strip():\n",
    "                    g.add((dataset, dcat.theme, URIRef(theme.strip())))\n",
    "            else: print(\"opendata.swiss theme was not formatted correctly\")\n",
    "    else: print(\"opendata.swiss theme was not provided\")\n",
    "    keys_DE = ODS_data.get(\"keywords_DE\").split(\";\")\n",
    "    if keys_DE[0]:\n",
    "        for key_DE in keys_DE:\n",
    "            if key_DE.strip():\n",
    "                g.add((dataset, dcat.keyword, Literal(key_DE.strip(), lang=\"de\")))\n",
    "            else: pass\n",
    "    else: print(\"opendata.swiss german keywords were not provided\")      \n",
    "    keys_FR = ODS_data.get(\"keywords_FR\").split(\";\")\n",
    "    if keys_FR[0]:\n",
    "        for key_FR in keys_FR:\n",
    "            if key_FR.strip():\n",
    "                g.add((dataset, dcat.keyword, Literal(key_FR.strip(), lang=\"fr\")))\n",
    "            else: pass\n",
    "    else: print(\"opendata.swiss french keywords were not provided\")  \n",
    "    keys_IT = ODS_data.get(\"keywords_IT\").split(\";\")\n",
    "    if keys_IT[0]:\n",
    "        for key_IT in keys_IT:\n",
    "            if key_IT.strip():\n",
    "                g.add((dataset, dcat.keyword, Literal(key_IT.strip(), lang=\"it\")))\n",
    "            else: pass    \n",
    "    else: print(\"opendata.swiss italian keywords were not provided\")  \n",
    "    keys_EN = ODS_data.get(\"keywords_EN\").split(\";\")\n",
    "    if keys_EN[0]:\n",
    "        for key_EN in keys_EN:\n",
    "            if key_EN.strip():\n",
    "                g.add((dataset, dcat.keyword, Literal(key_EN.strip(), lang=\"en\")))\n",
    "            else: pass\n",
    "    else: print(\"opendata.swiss english keywords were not provided\")\n",
    "    \n",
    "    langs = ODS_data.get(\"language\").split(\";\")\n",
    "    if langs[0]:\n",
    "        for language in langs:\n",
    "            if language:\n",
    "                language = language.strip()\n",
    "                g.add((dataset, dct.language, Literal(language, lang=language)))\n",
    "            else: pass \n",
    "    else: print(\"opendata.swiss languages missing\")\n",
    "\n",
    "    page = ODS_data.get(\"landing page\")\n",
    "    if not page: print(\"opendata.swiss landing page missing\")\n",
    "    elif \"https://\" in page:\n",
    "        g.add((dataset, dcat.landingPage, URIRef(page)))\n",
    "    else: print(\"opendata.swiss landing page not formatted correctly\")\n",
    "\n",
    "    docu = ODS_data.get(\"documentation\").split(\";\")\n",
    "    if docu[0]:\n",
    "        for doc in docu:\n",
    "            if not doc: pass\n",
    "            else: \n",
    "                g.add((dataset, URIRef(\"http://xmlns.com/foaf/0.1/page\"), Literal(doc.strip(), datatype=\"http://xmlns.com/foaf/0.1/Document\")))\n",
    "    else: \n",
    "        print(\"Optional: no opendata.swiss documentation pages provided\")\n",
    "    \n",
    "    rel_res = ODS_data.get(\"related resource\").split(\";\")\n",
    "    if rel_res[0]:\n",
    "        for res in rel_res:\n",
    "            if not res: pass\n",
    "            elif \"https://\" in res:\n",
    "                g.add((dataset, dct.relation, Literal(res.strip(), datatype=\"http://www.w3.org/2000/01/rdf-schema#Resource\")))\n",
    "            else: print(\"Optional: opendata.swiss related resource not formatted correctly\")\n",
    "    else: print(\"Optional: no opendata.swiss related resource provided\")\n",
    "    \n",
    "    qual_rel = ODS_data.get(\"qualified relation\").split(\";\")\n",
    "    if qual_rel[0]:\n",
    "        for rel in qual_rel:\n",
    "            if not rel: pass\n",
    "            elif \"https://\" in rel:\n",
    "                rel_BN = BNode()\n",
    "                g.add((dataset, dcat.qualifiedRelation, rel_BN))\n",
    "                g.add((rel_BN, RDF.type, dcat.Relationship))\n",
    "                g.add((rel_BN, dcat.hadRole, schema.sameAs))\n",
    "                g.add((rel_BN, dct.relation, Literal(rel.strip(), datatype=\"http://www.w3.org/2000/01/rdf-schema#Resource\")))\n",
    "            else: print(\"Optional: opendata.swiss qualified relation not formatted correctly\")\n",
    "    else: print(\"Optional: no opendata.swiss qualified relation provided\")\n",
    "    \n",
    "    temp_start = ODS_data.get(\"temporal start\").split(\";\")\n",
    "    if temp_start[0]:\n",
    "        for tem_sta in temp_start:\n",
    "            if not tem_sta: pass\n",
    "            try: \n",
    "                datetime.strptime(str(tem_sta).strip(), date_format)\n",
    "                g.add((dataset, schema.dateCreated, Literal(tem_sta, datatype=URIRef('http://www.w3.org/2001/XMLSchema#date'))))\n",
    "            except(ValueError):\n",
    "                print(\"Optional: opendata.swiss temporal start not formatted correctly\")\n",
    "    else: print(\"Optional: no opendata.swiss temporal start provided\")\n",
    "    temp_end = ODS_data.get(\"temporal end\").split(\";\")\n",
    "    if temp_end[0]:\n",
    "        for tem_end in temp_end:\n",
    "            if not tem_end: pass\n",
    "            try: \n",
    "                datetime.strptime(str(tem_end).strip(), date_format)\n",
    "                g.add((dataset, schema.dateCreated, Literal(tem_end, datatype=URIRef('http://www.w3.org/2001/XMLSchema#date'))))\n",
    "            except(ValueError):\n",
    "                print(\"Optional: opendata.swiss temporal end not formatted correctly\")\n",
    "    else: print(\"Optional: no opendata.swiss temporal end provided\")\n",
    "    \n",
    "    spatial = ODS_data.get(\"spatial\")\n",
    "    if spatial:\n",
    "        g.add((dataset, dct.spatial, Literal(spatial, lang = \"en\")))\n",
    "    else: print(\"Optional: opendata.swiss spatial information missing\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# Dimensions:\n",
    "\n",
    "g.add((dataset, cube.observationConstraint, dataset+\"/shape\"))\n",
    "#g.add((shape, shacl.property))\n",
    " \n",
    "shape=dataset+\"/shape\"\n",
    "\n",
    "g.add((shape, RDF.type, cube.Constraint))\n",
    "g.add((shape, RDF.type, shacl.NodeShape))\n",
    "g.add((shape, shacl.closed, Literal('true', datatype=URIRef(xsd.boolean))))\n",
    "\n",
    "#Dimension 1\n",
    "\n",
    "Bnode1 =BNode()\n",
    "g.add((shape, shacl.property, Bnode1))\n",
    "\n",
    "#One dimension is the observed by <an office> in every data point. Questionable yes, but standard.\n",
    "\n",
    "\n",
    "\n",
    "g.add((Bnode1, shacl.nodeKind,shacl.IRI ))\n",
    "g.add((Bnode1, URIRef(\"http://www.w3.org/ns/shacl#in\"),cube.Observation )) #sh.in does not work. Syntax error, don't know why\n",
    "g.add((Bnode1, schema.name,Literal(\"Beobachtet von\", lang=\"de\") ))\n",
    "g.add((Bnode1, schema.name,Literal(\"Observed by\", lang=\"en\") ))\n",
    "g.add((Bnode1, schema.name,Literal(\"Observ√© par\", lang=\"fr\") ))\n",
    "g.add((Bnode1, schema.name,Literal(\"Osservato da\", lang=\"it\") ))\n",
    "\n",
    "#g.add((Bnode1, schema.name, ))\n",
    "g.add((Bnode1, qudt.scaleType , qudt.NominalScale ))\n",
    "g.add((Bnode1, RDF.type, cube.KeyDimension))\n",
    "\n",
    "\n",
    "#All Dimensions\n",
    "\n",
    "\n",
    "\n",
    "num_dim=input_data.get(\"NumDimensions\")  #generate every dimension \n",
    "        \n",
    "d = {} #create all the blank nodes for all dimension\n",
    "for x in range(1, num_dim+1):\n",
    "    d[\"Bnode{0}\".format(x)] = BNode()        \n",
    "\n",
    "for dim in range(1,num_dim+1):\n",
    "    print(input_data.get((\"dimname_de\"+str(dim))))\n",
    "    print(d[(\"Bnode\"+str(dim))])\n",
    "    \n",
    "\n",
    "#for dim in range(1,num_dim+1):\n",
    "#    if input_data.get((\"dimname_de\"+str(dim))) :\n",
    " #       g.add((d[(\"Bnode\"+str(dim))], schema.name, Literal( input_data.get((\"dimname_de\"+str(dim))), lang=\"de\")))\n",
    "\n",
    "    \n",
    "for dim in range(1,num_dim+1): #todo: second for loop for languages + convert all to the \"if inputdata.get...\"\n",
    "    g.add((shape, shacl.property, d[(\"Bnode\"+str(dim))]))\n",
    "    g.add((d[(\"Bnode\"+str(dim))], schema.name, Literal( input_data.get((\"dimname_de\"+str(dim))), lang=\"de\")))\n",
    "    g.add((d[(\"Bnode\"+str(dim))], schema.description, Literal( input_data.get((\"dimdesscription_de\"+str(dim))), lang=\"de\")))\n",
    "    g.add((d[(\"Bnode\"+str(dim))], RDF.type, URIRef( input_data.get((\"dimtype\"+str(dim))))))\n",
    "    g.add((d[(\"Bnode\"+str(dim))], qudt.scaleType, URIRef( input_data.get((\"scaletype\"+str(dim))))))\n",
    "    if input_data.get((\"unit\"+str(dim))) :\n",
    "        g.add((d[(\"Bnode\"+str(dim))], qudt.unit, URIRef(input_data.get((\"unit\"+str(dim)))))) \n",
    "    if input_data.get((\"datatype\"+str(dim))) :\n",
    "        g.add((d[(\"Bnode\"+str(dim))], qudt.datatype, URIRef(input_data.get((\"datatype\"+str(dim))))))  \n",
    "    g.add((d[(\"Bnode\"+str(dim))], shacl.nodeKind, Literal( input_data.get((\"nodekind\"+str(dim))))))\n",
    "    #g.add((d[(\"Bnode\"+str(dim))], shacl.path, Pathi ))\n",
    "    \n",
    "#to do: at path of observation\n",
    "Path1= URIRef(dataset_URL+\"Jahr\")\n",
    "Path2= URIRef(dataset_URL+\"Region\")\n",
    "Path3= URIRef(dataset_URL+\"AnzahlgefundeneKleebl√§tter\")\n",
    "Path4= URIRef(dataset_URL+\"AnzahlLottogewinne\")\n",
    "\n",
    "\n",
    "# Observations\n",
    "\n",
    "df= pd.read_csv(\"C:\\\\Users\\\\claudio\\\\Documents\\\\Projekte\\\\python lottogewinne\\\\kleebl√§tter_lottogewinne.csv\")\n",
    "\n",
    "#add observations to datasetdescription \n",
    "g.add((dataset, cube.observationSet, dataset+\"/observation/\")) \n",
    "observation =dataset+\"/Observation/\"\n",
    "\n",
    "g.add((observation, RDF.type, cube.ObservationSet)) \n",
    "\n",
    "#generate the \"rows\" i.e. dataset_URL + the key-dimensions\n",
    "\n",
    "\n",
    "key1=df[\"Jahr\"]     #to do: fix this workaround for needing manually insert the right columnnames and automatcally generate as many keys are there are dimension in the yaml\n",
    "key2=df[\"Region\"]\n",
    "key3=df[\"Anzahl gefundener Kleebl√§tter\"]\n",
    "key4=df[\"Anzahl Lottogewinne\"]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    g.add((observation, cube.observation, URIRef(dataset_URL+str(key1[i])+\"/\" +  str(key2[i])))) #Connect all constructed \"rows\" to graph\n",
    "    g.add((URIRef(dataset_URL+str(key1[i])+\"/\" +  str(key2[i])), RDF.type, cube.Observation))\n",
    "    g.add((URIRef(dataset_URL+str(key1[i])+\"/\" +  str(key2[i])), cube.observedBy, Literal(\"Mein Amt\"))) #todo amt richtig ausf√ºllen\n",
    "    g.add((URIRef(dataset_URL+str(key1[i])+\"/\" +  str(key2[i])), Path1 , Literal(key1[i])))\n",
    "    g.add((URIRef(dataset_URL+str(key1[i])+\"/\" +  str(key2[i])), Path2 , Literal(key2[i])))\n",
    "    g.add((URIRef(dataset_URL+str(key1[i])+\"/\" +  str(key2[i])), Path3 , Literal(key3[i])))\n",
    "    g.add((URIRef(dataset_URL+str(key1[i])+\"/\" +  str(key2[i])), Path4 , Literal(key4[i])))\n",
    "    \n",
    "    \n",
    "    #print(dataset_URL+str(key1[i])+\"/\" +  str(key2[i]))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if name_DE: filename= name_DE.replace(\" \", \"_\").replace(\":\",\"\")+\".ttl\"\n",
    "elif name_FR: filename= name_FR.replace(\" \", \"_\").replace(\":\",\"\")+\".ttl\"\n",
    "elif name_IT: filename= name_IT.replace(\" \", \"_\").replace(\":\",\"\")+\".ttl\"\n",
    "else: filename= name_EN.replace(\" \", \"_\").replace(\":\",\"\")+\".ttl\"\n",
    "g.serialize(destination=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "355570a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "imname_de1: \"Jahr\"\n",
    "\n",
    "dimanme_fr1:\n",
    "dimdescription_de1: \"Loremipsum\"\n",
    "dimdescription_fr1:\n",
    "dimtype1: \"https://cube.link/KeyDimension\" #auch https://cube.link/MeasureDimension or empty\n",
    "scaletype1: \"http://qudt.org/schema/qudt/IntervalScale\" #to do find the other scale types, qudt:NominalScale\n",
    "unit1: \"http://qudt.org/vocab/unit/YR_Common\" #to do: show how to find all available on lindas (in a reasonable way)\n",
    "datatype1: \"http://www.w3.org/2001/XMLSchema#gYear\"\n",
    "#sh.max1\n",
    "#sh.min1\n",
    "nodeKind1: \"Literal\" # rdfs:comment \"The class of all node kinds, including sh:BlankNode, sh:IRI, sh:Literal or the combinations of these: sh:BlankNodeOrIRI, sh:BlankNodeOrLiteral, sh:IRIOrLiteral.\"@en ;\n",
    "path1: # URI of the Dimensionproperty in an observation. For example Year in https://energy.ld.admin.ch/sfoe/bfe_ogd84_einmalverguetung_fuer_photovoltaikanlagen/9/observation/2014/AG\n",
    "dataKind1: \"Year\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "44e8f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Jahr       Region  Anzahl gefundener Kleebl√§tter  Anzahl Lottogewinne  \\\n",
      "0    2017            1                            267                 30.7   \n",
      "1    2017            2                              9                  4.9   \n",
      "2    2017            3                             33                  7.3   \n",
      "3    2017            4                            277                 31.7   \n",
      "4    2017            5                            185                 22.5   \n",
      "..    ...          ...                            ...                  ...   \n",
      "157  2022           23                           2151                219.1   \n",
      "158  2022           24                           1277                131.7   \n",
      "159  2022           25                            104                 14.4   \n",
      "160  2022           26                           3560                360.0   \n",
      "161  2022  Switzerland                          28533               2857.3   \n",
      "\n",
      "     Kleeblatt Hotspot ID  \n",
      "0                       1  \n",
      "1                       2  \n",
      "2                       3  \n",
      "3                       4  \n",
      "4                       5  \n",
      "..                    ...  \n",
      "157                   158  \n",
      "158                   159  \n",
      "159                   160  \n",
      "160                   161  \n",
      "161                   162  \n",
      "\n",
      "[162 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import RDF\n",
    "from dateutil.parser import parse\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#instantiate namespace\n",
    "schema = Namespace(\"http://schema.org/\")\n",
    "dcat = Namespace(\"http://www.w3.org/ns/dcat#\")\n",
    "dct = Namespace(\"http://purl.org/dc/terms/\")\n",
    "void= Namespace(\"http://rdfs.org/ns/void#\")\n",
    "cube= Namespace(\"https://cube.link/\")\n",
    "shacl= Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "xsd= Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "qudt= Namespace(\"http://qudt.org/schema/qudt/\")\n",
    "\n",
    "#parse input\n",
    "with open(\"C:\\\\Users\\\\claudio\\\\Documents\\\\Projekte\\\\python lottogewinne\\\\input_Form.yml\", \"rt\", encoding='utf8') as yml_input:\n",
    "    input_data = yaml.load(yml_input, yaml.Loader)\n",
    "\n",
    "df= pd.read_csv(\"C:\\\\Users\\\\claudio\\\\Documents\\\\Projekte\\\\python lottogewinne\\\\kleebl√§tter_lottogewinne.csv\")\n",
    "\n",
    "print(df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c012a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\claudio\\\\Documents\\\\Projekte\\\\python lottogewinne',\n",
       " 'C:\\\\Users\\\\claudio\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\Users\\\\claudio\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\claudio\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\claudio\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\claudio\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\claudio\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\claudio\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\claudio\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "d\n",
    "\n",
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import RDF\n",
    "from dateutil.parser import parse\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g.add((Bnode1, schema.name,Literal(\"Beobachtet von\", lang=\"de\") ))\n",
    "g.add((Bnode1, schema.name,Literal(\"Observed by\", lang=\"en\") ))\n",
    "g.add((Bnode1, schema.name,Literal(\"Observ√© par\", lang=\"fr\") ))\n",
    "g.add((Bnode1, schema.name,Literal(\"Osservato da\", lang=\"it\") ))\n",
    "g.add((Bnode1, schema.name, ))\n",
    "g.add((Bnode1, qudt.scaleType,NominalScale  .))\n",
    "g.add((Bnode1, RDF.Type, cube.KeyDimension))\n",
    "\n",
    "\n",
    "<https://environment.ld.admin.ch/foen/template_pipeline/1/shape/> a sh:NodeShape, cube:Constraint ;\n",
    "\n",
    "    sh:closed true ;\n",
    "\n",
    "    sh:property [ schema:name \"Beobachtet von\"@de,\n",
    "\n",
    "                \"Observed by\"@en,\n",
    "\n",
    "                \"Observ√© par\"@fr,\n",
    "\n",
    "                \"Osservato da\"@it ;\n",
    "\n",
    "            qudt:scaleType qudt:NominalScale ;\n",
    "\n",
    "            sh:in ( <https://environment.ld.admin.ch/> ) ;\n",
    "\n",
    "            sh:nodeKind sh:IRI ;\n",
    "\n",
    "            sh:path cube:observedBy ],\n",
    "\n",
    "               [ a cube:KeyDimension ;\n",
    "\n",
    "            schema:name \"Warnregion\"@de,\n",
    "\n",
    "                \"Warning region\"@en,\n",
    "\n",
    "                \"R√©gion d‚Äôalerte\"@fr,\n",
    "\n",
    "                \"Regione di pericolo\"@it ;\n",
    "\n",
    "            qudt:scaleType qudt:NominalScale ;\n",
    "\n",
    "            sh:nodeKind sh:IRI ;\n",
    "\n",
    "           sh:maxCount 1 ;\n",
    "\n",
    "            sh:minCount 1 ;\n",
    "\n",
    "            sh:in (REGIONSLISTTOREPLACE) ;\n",
    "\n",
    "            meta:dataKind [a <http://schema.org/GeoShape>] ;\n",
    "\n",
    "            sh:path <https://environment.ld.admin.ch/foen/template_pipeline/canton> ],\n",
    "\n",
    "        [ a cube:MeasureDimension ;\n",
    "\n",
    "            schema:name \"Gefahrenstufe\"@de,\n",
    "\n",
    "                \"Danger ratings\"@en,\n",
    "\n",
    "                \"Degr√© de danger\"@fr,\n",
    "\n",
    "                \"Grado di pericolo\"@it ;\n",
    "\n",
    "            qudt:scaleType qudt:OrdinalScale ;\n",
    "\n",
    "                       sh:nodeKind sh:IRI ;\n",
    "\n",
    "            sh:maxCount 1 ;\n",
    "\n",
    "            sh:minCount 1 ;\n",
    "\n",
    "            sh:in (LEVELSLISTTOREPLACE) ;\n",
    "\n",
    "            sh:path <https://environment.ld.admin.ch/foen/template_pipeline/level> ] ,\n",
    "\n",
    "        [ schema:name \"Aktiv seit\"@de,\n",
    "\n",
    "                \"Active since\"@en,\n",
    "\n",
    "                \"Actif depuis\"@fr,\n",
    "\n",
    "                \"Attivo dal\"@it ;\n",
    "\n",
    "            qudt:scaleType qudt:IntervalScale ;\n",
    "\n",
    "            sh:nodeKind sh:Literal ;\n",
    "\n",
    "            sh:path schema:validFrom ]\n",
    "\n",
    "\n",
    "\n",
    "<http://www.w3.org/ns/shacl#path> <https://health.ld.admin.ch/foph/covid19/dimension/date> .\n",
    "<http://www.w3.org/ns/shacl#nodeKind> <http://www.w3.org/ns/shacl#Literal> .\n",
    "<http://www.w3.org/ns/shacl#datatype> <http://www.w3.org/2001/XMLSchema#date> .\n",
    "# <http://www.w3.org/ns/shacl#minCount> \"1\"^^<http://www.w3.org/2001/XMLSchema#integer> .\n",
    "#<http://www.w3.org/ns/shacl#maxCount> \"1\"^^<http://www.w3.org/2001/XMLSchema#integer> .\n",
    "# <http://www.w3.org/ns/shacl#minInclusive> \"2020-12-21\"^^<http://www.w3.org/2001/XMLSchema#date> .\n",
    "#<http://www.w3.org/ns/shacl#maxInclusive> \"2023-07-03\"^^<http://www.w3.org/2001/XMLSchema#date> .\n",
    " \n",
    " <https://cube.link/meta/dataKind> _:genid2dd5a88957033b4b0d8ada88adbe8ccae72d1b110 .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
